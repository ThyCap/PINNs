{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.autograd as autograd\n",
    "\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FCN import FCN\n",
    "from tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_diff_equation(f, g):\n",
    "    f_x_y = autograd.grad(f,g,torch.ones([g.shape[0], 1]), retain_graph=True, create_graph=True)[0] #first derivative\n",
    "    f_xx_yy = autograd.grad(f_x_y,g,torch.ones(g.shape), create_graph=True)[0]#second derivative\n",
    "\n",
    "    f_yy = f_xx_yy[:,[1]] # we select the 2nd element for y (the first one is x) (Remember the input X=[x,y]) \n",
    "    f_xx = f_xx_yy[:,[0]] # we select the 1st element for x (the second one is y) (Remember the input X=[x,y])\n",
    "\n",
    "    u = f_xx + f_yy # loss equation\n",
    "    u = u.float()\n",
    "\n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate2D(f, x_min = 0, x_max = 1, dx = 1e-3, y_min = 0, y_max = 1, dy = 1e-3):\n",
    "    x , y = x_min, y_min\n",
    "    integral = 0\n",
    "    area = 0\n",
    "\n",
    "    while x < x_max:\n",
    "        while y < y_max:\n",
    "            integral += f(x,y)*dx*dy\n",
    "            area += dx*dy\n",
    "\n",
    "            y += dy\n",
    "        \n",
    "        y = 0\n",
    "        x += dx\n",
    "    \n",
    "    return integral/area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exampleF(x, y):\n",
    "    return np.exp(-(x**2 + y**2)/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9933742443350323"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "integrate2D(exampleF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Setup and calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thyagocapitanio/anaconda3/lib/python3.10/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_2a19nf9hj1/croot/pytorch_1675190251927/work/aten/src/ATen/native/TensorShape.cpp:2895.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "myProblem = Problem(partial_diff_equation, squareHasHole = False, hasInternalHeat = False, evolutiveWeights= False)\n",
    "\n",
    "X_train_PDE, X_train_Nu, T_train_Nu, X_test = myProblem.getDomains()\n",
    "\n",
    "X_train, T_train = myProblem.X_train, myProblem.T_train\n",
    "\n",
    "x = myProblem.X[:,0]\n",
    "y = myProblem.Y[0,:]\n",
    "\n",
    "N_x, N_y = myProblem.N_x, myProblem.N_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PINN = FCN(myProblem, X_train_PDE, X_train, T_train, X_test, partial_diff_equation)\n",
    "PINN.load_state_dict(torch.load('./PINN_files/PINN_simple.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_PDE(FCN, x_PDE):\n",
    "        x_PDE = torch.from_numpy(x_PDE)\n",
    "\n",
    "        g = x_PDE.clone()\n",
    "        g.requires_grad = True\n",
    "\n",
    "        f = FCN.forward(g)\n",
    "\n",
    "        u = FCN.partial_diff_equation(f, g)\n",
    "\n",
    "        print(u)\n",
    "\n",
    "        u_hat = torch.zeros(x_PDE.shape[0],1)  \n",
    "        u_hat = u_hat.float()\n",
    "\n",
    "        loss = FCN.loss_function(u, u_hat)\n",
    "\n",
    "        FCN.loss_pde_history.append(loss.item())\n",
    "\n",
    "        return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0023],\n",
      "        [ 0.0003],\n",
      "        [-0.0002],\n",
      "        ...,\n",
      "        [ 0.0024],\n",
      "        [ 0.0017],\n",
      "        [-0.0004]], dtype=torch.float32, grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.0630e-06, dtype=torch.float32, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_PDE(PINN, X_train_PDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Loss2D(FCN):\n",
    "    def lossFunction(x, y):\n",
    "        g = torch.from_numpy(np.array([[x, y]], dtype = float))\n",
    "        g.requires_grad = True\n",
    "        f = FCN.forward(g)\n",
    "\n",
    "        f_x_y = autograd.grad(f, g, torch.ones([g.shape[0],1]), retain_graph=True, create_graph=True)[0] #first derivative\n",
    "        f_xx_yy = autograd.grad(f_x_y, g, torch.ones(g.shape), create_graph=True)[0]#second derivative\n",
    "\n",
    "        f_yy = f_xx_yy[:,[1]] # we select the 2nd element for y (the first one is x) (Remember the input X=[x,y]) \n",
    "        f_xx = f_xx_yy[:,[0]] # we select the 1st element for x (the second one is y) (Remember the input X=[x,y])\n",
    "\n",
    "        return (f_xx + f_yy)**2\n",
    "\n",
    "    return integrate2D(lossFunction)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Loss2D(PINN)\n",
      "Cell \u001b[0;32mIn[45], line 15\u001b[0m, in \u001b[0;36mLoss2D\u001b[0;34m(FCN)\u001b[0m\n\u001b[1;32m     11\u001b[0m     f_xx \u001b[39m=\u001b[39m f_xx_yy[:,[\u001b[39m0\u001b[39m]] \u001b[39m# we select the 1st element for x (the second one is y) (Remember the input X=[x,y])\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m (f_xx \u001b[39m+\u001b[39m f_yy)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\n\u001b[0;32m---> 15\u001b[0m \u001b[39mreturn\u001b[39;00m integrate2D(lossFunction)\n",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m, in \u001b[0;36mintegrate2D\u001b[0;34m(f, x_min, x_max, dx, y_min, y_max, dy)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mwhile\u001b[39;00m x \u001b[39m<\u001b[39m x_max:\n\u001b[1;32m      7\u001b[0m     \u001b[39mwhile\u001b[39;00m y \u001b[39m<\u001b[39m y_max:\n\u001b[0;32m----> 8\u001b[0m         integral \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m f(x,y)\u001b[39m*\u001b[39mdx\u001b[39m*\u001b[39mdy\n\u001b[1;32m      9\u001b[0m         area \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m dx\u001b[39m*\u001b[39mdy\n\u001b[1;32m     11\u001b[0m         y \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m dy\n",
      "Cell \u001b[0;32mIn[45], line 7\u001b[0m, in \u001b[0;36mLoss2D.<locals>.lossFunction\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m      4\u001b[0m g\u001b[39m.\u001b[39mrequires_grad \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m      5\u001b[0m f \u001b[39m=\u001b[39m FCN\u001b[39m.\u001b[39mforward(g)\n\u001b[0;32m----> 7\u001b[0m f_x_y \u001b[39m=\u001b[39m autograd\u001b[39m.\u001b[39;49mgrad(f, g, torch\u001b[39m.\u001b[39;49mones([g\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m],\u001b[39m1\u001b[39;49m]), retain_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, create_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)[\u001b[39m0\u001b[39m] \u001b[39m#first derivative\u001b[39;00m\n\u001b[1;32m      8\u001b[0m f_xx_yy \u001b[39m=\u001b[39m autograd\u001b[39m.\u001b[39mgrad(f_x_y, g, torch\u001b[39m.\u001b[39mones(g\u001b[39m.\u001b[39mshape), create_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m0\u001b[39m]\u001b[39m#second derivative\u001b[39;00m\n\u001b[1;32m     10\u001b[0m f_yy \u001b[39m=\u001b[39m f_xx_yy[:,[\u001b[39m1\u001b[39m]] \u001b[39m# we select the 2nd element for y (the first one is x) (Remember the input X=[x,y]) \u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/autograd/__init__.py:276\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[39mreturn\u001b[39;00m _vmap_internals\u001b[39m.\u001b[39m_vmap(vjp, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, allow_none_pass_through\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    275\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 276\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    277\u001b[0m         t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,\n\u001b[1;32m    278\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Loss2D(PINN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = PINN.forward(X_test)\n",
    "u_pred = np.transpose(np.reshape(u_pred.detach().numpy(), (N_x, N_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossTensor = PINN.lossTensor(X_test)\n",
    "lossTensor = np.reshape(lossTensor.detach().numpy(), (N_x,N_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
